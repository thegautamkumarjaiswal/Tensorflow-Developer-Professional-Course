# Course 1: Natural Language Processing in TensorFlow

In this third course, you’ll learn how to apply neural networks to solve natural language processing problems using TensorFlow. You’ll learn how to process and represent text through tokenization so that it’s recognizable by a neural network. You’ll be introduced to new types of neural networks, including RNNs, GRUs and LSTMs, and how you can train them to understand the meaning of text. Finally, you’ll learn how to train LSTMs on existing text to create original poetry and more!

#### Week 1: Sentiment in Text

- Introduction: A conversation with Andrew Ng
-  Word-based encodings
- Using APIs
- Text to sequence
- Sarcasm, really?
- Working with the Tokenizer
- Lab Excercise - Exploring BBC news data

#### Week 2: Word Embeddings

- A conversation with Andrew Ng
- The IMDB dataset
- Looking into the details
- How can we use vectors?
- More into the details
- Remember the sarcasm dataset?
- Building a classifier for the sarcasm dataset
- Let’s talk about the loss function
- Pre-tokenized datasets
- Diving into the code
- Lab Excercise - Classifying BBC news into topics (Embedding + Conv + MLP)

#### Week 3: Sequence Models

- A conversation with Andrew Ng
- LSTMs
- Implementing LSTMs in code
- A word from Laurence
- Accuracy and Loss
- Using a convolutional network
- Going back to the IMDB dataset
- Tips from Laurence
- Lab Excercise - Twitter sentiment classification (EXploring overfitting in NLP)

#### Week 4: Sequence Models and Literature

- A conversation with Andrew Ng
- Training the data
- Finding what the next word should be
- Predicting a word
- Poetry!
- Laurence the poet
- Lab Excercise - Poem generation with Bi-directional LSTM


## Here is My Certificate link: https://coursera.org/share/9ca43aa80a22029181527e47b7027f31
